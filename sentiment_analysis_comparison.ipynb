{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Comparison for Coldplay Lyrics\n",
    "\n",
    "This notebook performs sentiment analysis on Coldplay song lyrics using multiple models and compares their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Step 1: Data Loading and Preprocessing\")\n",
    "try:\n",
    "    df = pd.read_excel('../input/coldplay-research-project-data/Coldplay Research Project_Data.xlsx')\n",
    "    logging.info(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Preprocess the text data\n",
    "df['lyrics_clean'] = df['Lyrics'].str.lower().str.replace(r'[^\\w\\s]', '')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Step 2: Model Initialization\")\n",
    "# Check if GPU is available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "logging.info(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n",
    "\n",
    "# Initialize sentiment analysis pipelines and tokenizers for different models\n",
    "models = {}\n",
    "tokenizers = {}\n",
    "model_configs = [\n",
    "    ('siebert/sentiment-roberta-large-english', 'sentiment-analysis'),\n",
    "    ('cardiffnlp/twitter-roberta-base-sentiment', 'sentiment-analysis'),\n",
    "    ('jialicheng/electra-base-imdb', 'sentiment-analysis'),\n",
    "    ('textattack/albert-base-v2-SST-2', 'sentiment-analysis'),\n",
    "    ('dipawidia/xlnet-base-cased-product-review-sentiment-analysis', 'sentiment-analysis'),\n",
    "    ('nlptown/bert-base-multilingual-uncased-sentiment', 'sentiment-analysis'),\n",
    "    ('distilbert-base-uncased-finetuned-sst-2-english', 'sentiment-analysis')\n",
    "]\n",
    "\n",
    "for model_name, task in model_configs:\n",
    "    try:\n",
    "        models[model_name] = pipeline(task, model=model_name, device=device)\n",
    "        tokenizers[model_name] = AutoTokenizer.from_pretrained(model_name)\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        logging.warning(f\"GPU out of memory for {model_name}. Falling back to CPU.\")\n",
    "        models[model_name] = pipeline(task, model=model_name, device=-1)\n",
    "        tokenizers[model_name] = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "print(\"Models initialized:\")\n",
    "for model_name in models.keys():\n",
    "    print(f\"- {model_name}\")\n",
    "print(\"- VADER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Step 3: Sentiment Analysis\")\n",
    "# Function to get VADER sentiment\n",
    "def get_vader_sentiment(text):\n",
    "    return vader.polarity_scores(text)['compound']\n",
    "\n",
    "# Function to get Hugging Face transformer sentiment\n",
    "def get_transformer_sentiment(model_pipeline, tokenizer, text):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        result = model_pipeline(inputs)[0]\n",
    "        return result['label'], result['score']\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in sentiment analysis: {e}\")\n",
    "        return \"ERROR\", 0.0\n",
    "\n",
    "# Function to normalize sentiment scores\n",
    "def normalize_sentiment_score(score, model_name):\n",
    "    if 'roberta' in model_name.lower():\n",
    "        return (score - 1) / 4  # RoBERTa models typically output 0-4\n",
    "    elif 'albert' in model_name.lower() or 'electra' in model_name.lower() or 'bert' in model_name.lower():\n",
    "        return score  # These models typically output 0-1\n",
    "    elif 'xlnet' in model_name.lower():\n",
    "        return (score * 2) - 1  # XLNet typically outputs 0-1, convert to -1 to 1\n",
    "    else:\n",
    "        return score  # Default case\n",
    "\n",
    "# Function to calculate average sentiment score\n",
    "def calculate_average_sentiment(df, model_name):\n",
    "    return df[f'{model_name}_normalized_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply each model to the dataset and store results\n",
    "for model_name, model_pipeline in tqdm(models.items(), desc=\"Analyzing sentiments\"):\n",
    "    df[f'{model_name}_sentiment'], df[f'{model_name}_score'] = zip(*df['lyrics_clean'].apply(lambda x: get_transformer_sentiment(model_pipeline, tokenizers[model_name], x)))\n",
    "    df[f'{model_name}_normalized_score'] = df[f'{model_name}_score'].apply(lambda x: normalize_sentiment_score(x, model_name))\n",
    "    avg_sentiment = calculate_average_sentiment(df, model_name)\n",
    "    logging.info(f\"Average sentiment for {model_name}: {avg_sentiment:.4f}\")\n",
    "\n",
    "# Apply VADER to the dataset\n",
    "df['vader_sentiment'] = df['lyrics_clean'].apply(get_vader_sentiment)\n",
    "vader_avg = df['vader_sentiment'].mean()\n",
    "logging.info(f\"Average VADER sentiment: {vader_avg:.4f}\")\n",
    "\n",
    "# Display the first few rows of the dataframe with sentiment scores\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Step 5: Results Processing\")\n",
    "\n",
    "# Create a results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Track Name': df['Track Name'],\n",
    "    'Album Name': df['Album Name'],\n",
    "    'Album Release Date': df['Album Release Date']\n",
    "})\n",
    "\n",
    "# Add sentiment scores for each model\n",
    "for model_name in models.keys():\n",
    "    results_df[f'{model_name}_sentiment'] = df[f'{model_name}_normalized_score']\n",
    "\n",
    "# Add VADER sentiment\n",
    "results_df['VADER_sentiment'] = df['vader_sentiment']\n",
    "\n",
    "# Calculate average sentiment across all models\n",
    "model_columns = [f'{model}_sentiment' for model in models.keys()] + ['VADER_sentiment']\n",
    "results_df['Average_sentiment'] = results_df[model_columns].mean(axis=1)\n",
    "\n",
    "# Sort the results by average sentiment\n",
    "results_df = results_df.sort_values('Average_sentiment', ascending=False)\n",
    "\n",
    "# Display the top 10 most positive and most negative songs\n",
    "print(\"\\nTop 10 Most Positive Songs:\")\n",
    "display(results_df.head(10)[['Track Name', 'Album Name', 'Album Release Date', 'Average_sentiment']])\n",
    "\n",
    "print(\"\\nTop 10 Most Negative Songs:\")\n",
    "display(results_df.tail(10)[['Track Name', 'Album Name', 'Album Release Date', 'Average_sentiment']])\n",
    "\n",
    "# Calculate average sentiment by album\n",
    "album_sentiment = results_df.groupby('Album Name')['Average_sentiment'].mean().sort_values(ascending=False)\n",
    "print(\"\\nAverage Sentiment by Album:\")\n",
    "display(album_sentiment)\n",
    "\n",
    "# Calculate average sentiment by release date\n",
    "release_date_sentiment = results_df.groupby('Album Release Date')['Average_sentiment'].mean().sort_values(ascending=False)\n",
    "print(\"\\nAverage Sentiment by Album Release Date:\")\n",
    "display(release_date_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots to visualize the sentiment distributions\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 14))\n",
    "\n",
    "# Plot for Hugging Face models\n",
    "for model_name in models.keys():\n",
    "    sns.histplot(data=df, x=f'{model_name}_normalized_score', kde=True, ax=ax1, label=model_name, alpha=0.5)\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.set_title('Hugging Face Models Sentiment Score Distributions')\n",
    "ax1.set_xlabel('Normalized Sentiment Score')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "# Plot for VADER\n",
    "sns.histplot(data=df, x='vader_sentiment', kde=True, ax=ax2, label='VADER', alpha=0.5)\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_title('VADER Sentiment Score Distribution')\n",
    "ax2.set_xlabel('Sentiment Score')\n",
    "ax2.set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize average sentiment by album\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=album_sentiment.index, y=album_sentiment.values)\n",
    "plt.title('Average Sentiment by Album')\n",
    "plt.xlabel('Album Name')\n",
    "plt.ylabel('Average Sentiment')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize sentiment over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(data=results_df, x='Album Release Date', y='Average_sentiment')\n",
    "plt.title('Sentiment Trend Over Time')\n",
    "plt.xlabel('Album Release Date')\n",
    "plt.ylabel('Average Sentiment')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the results to a CSV for further analysis\n",
    "results_df.to_csv('sentiment_analysis_results.csv', index=False)\n",
    "logging.info(\"Results exported to sentiment_analysis_results.csv\")\n",
    "\n",
    "logging.info(\"Sentiment classification process completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name":
